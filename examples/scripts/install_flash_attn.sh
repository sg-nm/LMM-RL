# install flash attention2.0
pip uninstall transformer-engine
pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.1.1/flash_attn-2.1.1+cu121torch2.1cxx11abiTRUE-cp310-cp310-linux_x86_64.whl